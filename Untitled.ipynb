{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Here we'll generate syntetic image using PIL and system wide fonts. The make_img helper function generate.\n",
    "Since our network will only accept fixed size input, we hardcode a 105 x 105 size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "def make_img(text, font):\n",
    "  img = PIL.Image.new(\"RGB\", (105, 105))\n",
    "  draw = PIL.ImageDraw.Draw(img)\n",
    "  font = PIL.ImageFont.truetype(font, size=45)\n",
    "  draw.text((0, 0), text, font = font)\n",
    "  return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's see what kind of image it produces. Feel free to replace the true type font file according to your system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071f5475f51e4cc1a935fc84084048a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x189850dfa90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyplot.imshow(make_img(\"test\", \"C:\\\\windows\\\\fonts\\\\arial.ttf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's obviously room for improvement..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation dataset are passed through a Dataset interface. Such interface needs to implement 3 members functions, __init__(), __len__() and __get_item__(idx) which are rather self explanatory.\n",
    "Our implementation will create a random 4 letter text and pick a font and returns a text image along with the font index (data needs to be labeled by their class).\n",
    "\n",
    "By the way we use the name VFR for Visual Font Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_lowercase\n",
    "import random\n",
    "from random import choice\n",
    "import os\n",
    "import torch.utils.data\n",
    "\n",
    "random.seed(0)    \n",
    "class CustomVFR(torch.utils.data.Dataset):\n",
    "  def get_word(self):\n",
    "    return ''.join([choice(ascii_lowercase) for _ in range(4)])\n",
    "  def __init__(self, size, transform = None):\n",
    "    self.transform = transform\n",
    "    self.fonts = []\n",
    "    self.size = size\n",
    "    for path, dir, files in os.walk(\"C:\\\\windows\\\\fonts\"):\n",
    "      for file in files:\n",
    "        if file.endswith(\".ttf\"):\n",
    "            self.fonts.append(file)\n",
    "    self.texts = [self.get_word() for _ in range(len(self.fonts) * self.size)]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.fonts) * self.size\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    idx = i % len(self.fonts)\n",
    "    word = self.texts[i]\n",
    "    img = make_img(word, self.fonts[idx])\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "    return img, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The network\n",
    "Now our network : it only needs to implement an __init__() and a forward(x) method and to inherit from the Module class.\n",
    "The x argument in the forward method is a torch Tensor object which will record gradient of their operator so that a backward operation is actually not needed.\n",
    "The Network structure comes from Adobe DeepFont paper : https://arxiv.org/abs/1507.03196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepFont(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DeepFont, self).__init__()\n",
    "    # unsupervisedly trained layers\n",
    "    self.conv_layer1 = torch.nn.Conv2d(1, 64, 9, stride=2)\n",
    "    self.lrn1 = torch.nn.LocalResponseNorm(2)\n",
    "    self.max_pooling1 = torch.nn.MaxPool2d(2, return_indices=True)\n",
    "    self.conv_layer2 = torch.nn.Conv2d(64, 128, 3, padding=1)\n",
    "    \n",
    "    self.deconv_layer1 = torch.nn.ConvTranspose2d(128, 64, 3, padding=1)\n",
    "    self.max_unpooling1 = torch.nn.MaxUnpool2d(2)\n",
    "    self.deconv_layer2 = torch.nn.ConvTranspose2d(64, 1, 11, stride=2)\n",
    "    \n",
    "    self.lrn2 = torch.nn.LocalResponseNorm(2)\n",
    "    self.max_pooling2 = torch.nn.MaxPool2d(2)\n",
    "\n",
    "    self.conv_layer3 = torch.nn.Conv2d(128, 256, 3, padding=1)\n",
    "    self.conv_layer4 = torch.nn.Conv2d(256, 256, 3, padding=1)\n",
    "    self.conv_layer5 = torch.nn.Conv2d(256, 256, 3, padding=1)\n",
    "    self.fc6 = torch.nn.Linear(256*12*12, 4096)\n",
    "    self.fc7 = torch.nn.Linear(4096, 2383)\n",
    "    self.fc8 = torch.nn.Linear(2383, 2383)\n",
    "    \n",
    "    self.unsupervised_learning = True\n",
    "    \n",
    "  def freeze_unsupervised(self):\n",
    "    self.unsupervised_learning = False\n",
    "    self.conv_layer1.requires_grad = False\n",
    "    self.lrn1.requires_grad = False\n",
    "    self.max_pooling1.requires_grad = False\n",
    "    self.conv_layer2.requires_grad = False\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.conv_layer1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.lrn1(x)\n",
    "    x, indexes = self.max_pooling1(x)\n",
    "\n",
    "    x = self.conv_layer2(x)\n",
    "    x = F.relu(x)\n",
    "    \n",
    "    if self.unsupervised_learning:\n",
    "        x = self.deconv_layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_unpooling1(x, indexes)\n",
    "        x = self.deconv_layer2(x)\n",
    "        return x\n",
    "    else:\n",
    "        x = self.lrn2(x)\n",
    "        x = self.max_pooling2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_layer4(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 256*12*12)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validation\n",
    "Instanciate our dataset : we can use the transforms mechanism to alter our data before passing them to a network. In our case we need to convert PIL image to torch tensor ; we could also rotate, crop or whatever images to increase robustness (see: data augmentation).\n",
    "As usual we use separate set for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trfrm = transforms.Compose( \n",
    "    [\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "trainingset = CustomVFR(100, transform = trfrm)\n",
    "testset = CustomVFR(10, transform = trfrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease dataset manipulation pytorch has a DataLoader object that can batch and shuffle data. Since pytorch network always take batch as input it's useful to rely on such object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingloader = torch.utils.data.DataLoader(trainingset, batch_size=128,\n",
    "                        shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the real part: we instanciate the network and train it using the cross entropy loss criterion and the Stochastic Gradient Descent optimisation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Res = DeepFont()\n",
    "Res.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(Res.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a small class to implement both progress bars (training can be long, it's better to have some feedback) and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressReport:\n",
    "    def __init__(self, training_set_count, figure_id, labels):\n",
    "        from ipywidgets import FloatProgress\n",
    "        from IPython import display\n",
    "\n",
    "        self.all_epoch_progress = FloatProgress(min=0, max=100, description='all epoch progress')\n",
    "        self.current_epoch_progress = FloatProgress(min=0, max=training_set_count, description='current epoch progress')\n",
    "        \n",
    "        display.display(self.all_epoch_progress)\n",
    "        display.display(self.current_epoch_progress)\n",
    "        \n",
    "        self.fig = fig = pyplot.figure(figure_id)\n",
    "        self.plots = []\n",
    "        for i, label in enumerate(labels):\n",
    "            ax = pyplot.subplot(len(labels), 1, i + 1)\n",
    "            ax.plot(range(100), range(100), label= label)\n",
    "            self.plots.append(ax)\n",
    "        self.epoch_data = []\n",
    "        \n",
    "    def mark_next_batch(self):\n",
    "        self.current_epoch_progress.value += 1\n",
    "        \n",
    "    def mark_epoch(self, value):\n",
    "          self.epoch_data.append(value)\n",
    "          self.current_epoch_progress.value = 0\n",
    "          self.all_epoch_progress.value += 1\n",
    "          for i, plt in enumerate(self.plots):\n",
    "              plt.lines[0].set_xdata(range(len(self.epoch_data)))\n",
    "              np_unsup_ep_loss = np.asarray([v[i] for v in self.epoch_data])\n",
    "              plt.lines[0].set_ydata(np_unsup_ep_loss)\n",
    "              plt.set_xlim(0, 100)\n",
    "              plt.set_ylim(np_unsup_ep_loss.min(), np_unsup_ep_loss.max() + 1)\n",
    "          self.fig.canvas.draw()\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.epoch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stacked Convolutional Auto Encoder training part  basically takes the first half of the network, plug it into some kind of reverse network and then train the whole new network to be as close as possible from an identity transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0938a89b18584ed1bec76bbc7c5e13cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='all epoch progress')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e7ac35ecfa41549691e538dd816cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='current epoch progress', max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ba5ae373c3487d83ad2fee728248c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'unsupervised_epoch_losses'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unsupervised_criterion = torch.nn.MSELoss()\n",
    "PR = ProgressReport(len(trainingloader), 2, ['loss'])\n",
    "#pyplot.yscale('log')\n",
    "for epoch in range(100):\n",
    "  epoch_loss = 0\n",
    "  for imgs, _ in trainingloader:\n",
    "    imgs = imgs.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = Res(imgs)\n",
    "\n",
    "    loss = unsupervised_criterion(outputs.type_as(imgs), imgs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()\n",
    "    PR.mark_next_batch()\n",
    "  PR.mark_epoch([epoch_loss])\n",
    "%store unsupervised_epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 100 epoch ; this means we'll pass training data 100 times in the network.\n",
    "For every epoch we run the network on our validation data to see how accurate the prediction is.\n",
    "The two value are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8547329ff63c486a99f60b2f3f2e1a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='all epoch progress')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f3584c6144758a6109a4cde13d1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='current epoch progress', max=11.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec08dac08544bf298241ea5c7208177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFreeTypeFont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout_engine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: cannot open resource",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-efc980a1db68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[0mRes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfonts\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m       \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a138dc73fb8c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfonts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfonts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m       \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2292963b1cb7>\u001b[0m in \u001b[0;36mmake_img\u001b[1;34m(text, font)\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m105\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m105\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[0mdraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageDraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m   \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m   \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mfirst_font_with_a_different_extension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mwalkroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalkdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalkfilenames\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mwalkfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwalkfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mwalkfilename\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mttf_filename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\pytorch\\lib\\os.py\u001b[0m in \u001b[0;36mwalk\u001b[1;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m                     \u001b[0mentry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Res.freeze_unsupervised()\n",
    "\n",
    "PR = ProgressReport(len(trainingloader) + len(testloader), 3, ['training', 'validation'])\n",
    "epoch_losses = []\n",
    "Res.train()\n",
    "for epoch in range(100):\n",
    "  epoch_loss = 0\n",
    "  correct = 0\n",
    "  all = 0\n",
    "  for imgs, fonts in trainingloader:\n",
    "    imgs, fontsgpu = imgs.to(device), fonts.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = Res(imgs)\n",
    "    \n",
    "    _, idx = torch.max(outputs, 1)\n",
    "    tmp = (idx.cpu() == fonts).sum().item()\n",
    "    correct += tmp\n",
    "    all += len(fonts)\n",
    "\n",
    "    loss = criterion(outputs, fontsgpu)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()\n",
    "    PR.mark_next_batch()\n",
    "  training_accuracy = correct/all\n",
    "  correct = 0\n",
    "  all = 0\n",
    "  Res.eval()\n",
    "  with torch.no_grad():\n",
    "    for imgs, fonts in testloader:\n",
    "      imgs = imgs.to(device)\n",
    "      outputs = Res(imgs)\n",
    "      _, idx = torch.max(outputs, 1)\n",
    "      tmp = (idx.cpu() == fonts).sum().item()\n",
    "      correct += tmp\n",
    "      all += len(fonts)\n",
    "      PR.mark_next_batch()\n",
    "  epoch_losses.append(epoch_loss)\n",
    "  test_accuracy = correct/all\n",
    "  PR.mark_epoch((training_accuracy, test_accuracy))\n",
    "%store epoch_training_accuracy\n",
    "%store epoch_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
